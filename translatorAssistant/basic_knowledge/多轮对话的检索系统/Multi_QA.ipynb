{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"你的openapi key\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"你的地址\"\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"你的langsmith key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43130\n"
     ]
    }
   ],
   "source": [
    "# 1.加载文档 使用 `WebBaseLoader` 类从指定来源加载内容，并生成 `Document` 对象（依赖 `bs4` 库）。\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 66 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# 分割\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to ./docs\n"
     ]
    }
   ],
   "source": [
    "# 3.进行向量化和存储\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def create_vector_store(chunks,save_path=''):\n",
    "    # 初始化 OpenAI 嵌入模型\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\",chunk_size=1000)\n",
    "    # 创建FAISS向量库\n",
    "    vector_store = FAISS.from_documents(documents=chunks,embedding=embedding)\n",
    "    \n",
    "    # 保存到本地（可选）\n",
    "    if save_path:\n",
    "        vector_store.save_local(save_path)\n",
    "        print(f\"Vector store saved to {save_path}\")\n",
    "    return vector_store\n",
    "\n",
    "vector_store = create_vector_store(all_splits,'./docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_store)\n",
    "print(f\"已加载 {len(vector_store)} 个文档块\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 66 个文档块\n",
      "<langchain_community.vectorstores.faiss.FAISS object at 0x135953990>\n"
     ]
    }
   ],
   "source": [
    "# 如果已经有embedding文件，直接加载，不要重新处理了，节省token\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\",chunk_size=1000)\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"./docs\",       # 存放index.faiss和index.pkl的目录路径\n",
    "    embeddings=embedding,      # 必须与创建时相同的嵌入模型\n",
    "    index_name=\"index\",          # 可选：若文件名不是默认的\"index\"，需指定前缀\n",
    "    allow_dangerous_deserialization=True  # 显式声明信任\n",
    ")\n",
    "print(f\"已加载 {len(vector_store.docstore._dict)} 个文档块\")\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# 使用rag的提示词模版 - 尝试可用性，真实代码不展示这段\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.准备和Retrieval链\n",
    "from langchain_openai import ChatOpenAI # ChatOpenAI模型\n",
    "\n",
    "# 实例化一个大模型工具 - OpenAI的GPT-3.5\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化记忆存储（保存对话历史）\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",  # 存储对话历史的键名\n",
    "    return_messages=True        # 以消息列表格式存储（适合ChatModel）\n",
    ")\n",
    "def save_memory(question,content):\n",
    "    memory.chat_memory.add_user_message(question)\n",
    "    memory.chat_memory.add_ai_message(content)  # 最后一块内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 问答展示\n",
    "from langchain import hub\n",
    "\n",
    "prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def ask_question(question,chat_history,usetream=False):\n",
    "    memory.clear()\n",
    "    # 1. 将历史对话加载到memory（如果是首次调用则跳过）\n",
    "    if chat_history:\n",
    "        for turn in chat_history:\n",
    "            print('chat_history',chat_history)\n",
    "            memory.chat_memory.add_user_message(turn[\"user\"])  # 用户问题\n",
    "            memory.chat_memory.add_ai_message(turn[\"ai\"])      # AI回答\n",
    "    # 2. 检索文档\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    # 3. 构建包含上下文的prompt\n",
    "    prompt = prompt_template.invoke({\"question\": question, \"context\": docs_content, \"chat_history\": memory.buffer})\n",
    "    # print(f'prompt:{prompt}')\n",
    "    # 4. 流式/普通模式处理\n",
    "    if usetream:\n",
    "        # 返回生成器，逐块 yield 内容\n",
    "        def stream_generator():\n",
    "            for chunk in llm.stream(prompt):\n",
    "                yield chunk.content\n",
    "            # 将本轮对话存入memory\n",
    "            save_memory(question, content=chunk)\n",
    "        return stream_generator()\n",
    "    else:\n",
    "        answer = llm.invoke(prompt)\n",
    "        # 保存到记忆\n",
    "        save_memory(question, content=answer.content)\n",
    "        return answer.content   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Chain of Thought (CoT) is a prompting technique that enhances model performance on complex tasks by instructing the model to \"think step by step.\" This approach decomposes difficult tasks into smaller, more manageable steps, allowing for better reasoning and interpretation of the model's thought process. CoT has become a standard method for improving the handling of intricate problems in language models.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第一轮对话\n",
    "history = []  # 初始化空历史\n",
    "q1 = \"What is Chain of thought ?\"\n",
    "answer1=ask_question(q1,chat_history=history)\n",
    "print(f'Answer: {answer1}\\n\\n')\n",
    "history.append({\"user\": q1, \"ai\": answer1})  # 记录到历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history [{'user': 'What is Chain of thought ?', 'ai': 'Chain of Thought (CoT) is a prompting technique that enhances model performance on complex tasks by instructing the model to \"think step by step.\" This approach decomposes difficult tasks into smaller, more manageable steps, allowing for better reasoning and interpretation of the model\\'s thought process. CoT has become a standard method for improving the handling of intricate problems in language models.'}]\n",
      "The content provided focuses on writing code based on detailed instructions and reasoning through decisions step by step. In contrast, Tree of Thoughts extends this approach by exploring multiple reasoning possibilities at each step, creating a tree structure for problem decomposition and thought generation. The search process in Tree of Thoughts can be BFS or DFS with evaluation by a classifier or majority vote."
     ]
    }
   ],
   "source": [
    "# 第二轮对话（携带上文）\n",
    "q2 = \"What is the difference between the above content and Tree of Thoughts?\"\n",
    "answer2 = ask_question(q2, chat_history=history, usetream=True)  # 流式输出\n",
    "for chunk in answer2:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "history.append({\"user\": q2, \"ai\": \"\".join(chunk)})  # 记录流式结果的拼接"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
